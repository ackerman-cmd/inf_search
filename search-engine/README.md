# Search Engine — отчёт о пайплайне работы

В этом каталоге реализован упрощённый пайплайн для построения булевого индекса и анализа частотности слов в корпусе. Ниже описано, как данные проходят через систему — от корпуса до индекса и поиска — и приведены команды для воспроизведения шагов.

## Основные модули (файлы в `src/`)
- `tokenizer.cpp` — токенизация корпуса, подсчёт частот, вывод `results/frequencies.csv` и `results/stats.txt`.
- `zipf_analyzer.py` — анализ распределения частот (Zipf / Mandelbrot), строит графики и сохраняет отчёт в `results/`.
- `boolean_index.cpp` — строит булев (инвертированный) индекс из дампа документов и сохраняет его в `data/boolean_index.idx`.
- `boolean_search.cpp` — загружает сохранённый индекс и выполняет интерактивный булев поиск (AND/OR/NOT).
- `simple_stemmer.cpp` — простая утилита-стеммер для предобработки корпуса (опционально).

## Пайплайн обработки — шаги

1. Подготовьте корпус (текстовый дамп). Для `boolean_index.cpp` дамп должен содержать документы в следующем формате:

```
==DOC_START==
<external_id_or_title>
==CONTENT_START==
... content lines ...
==DOC_END==
```

2. Токенизация и подсчёт частот:
   - Запустите `tokenizer.cpp`. Он читает корпус (stdin или файл), извлекает токены (алфанумерические последовательности длиной >=2), считает частоты и записывает `results/frequencies.csv`.
   - В `results/stats.txt` появляется статистика (токены, уникальные слова, время).

3. Анализ закона Ципфа (опционально):
   - `zipf_analyzer.py` читает `results/frequencies.csv`, подгоняет Zipf и Mandelbrot, строит графики `results/zipf_mandelbrot.png`/`.pdf` и сохраняет `results/zipf_analysis.txt`.

4. Построение булевого индекса:
   - `boolean_index.cpp` читает дамп и строит инвертированный индекс: для каждого терма — список doc_id.
   - Результат сохраняется в `data/boolean_index.idx` в формате с секциями `DOCS` и `TERMS`.

5. Поиск по индексу:
   - `boolean_search.cpp` загружает `data/boolean_index.idx` и предоставляет интерактивную консоль для запросов.

## Запуск (автоматизированный)

Вместо ручного выполнения отдельных команд для компиляции, токенизации, анализа и построения индекса, используйте существующий скрипт `run_all.sh` в корне каталога — он автоматизирует последовательность шагов пайплайна (включая вызов `compile.sh`).

Откройте `run_all.sh`, чтобы посмотреть точную последовательность выполняемых действий и при необходимости внести локальные правки (пути к файлам, опции). Скрипт уже содержит все команды для полного прогона пайплайна: компиляция, токенизация, анализ Ципфа, построение индекса и запуск интерактивного поиска.

## Форматы данных и результаты
- Входной дамп: документные блоки с маркерами (`==DOC_START==`, `==CONTENT_START==`, `==DOC_END==`).
- `results/frequencies.csv`: CSV с колонками `Rank,Frequency,Word` (генерируется `tokenizer`).
- `results/stats.txt`: время выполнения, число токенов, уникальные слова, средняя длина токена.
- `data/boolean_index.idx`: индекс с секциями `DOCS` (список doc_id|title|preview) и `TERMS` (term|doc1,doc2,...).

## Важные детали реализации
- Токенизация: только буквенно-цифровые символы рассматриваются как часть токена; все токены приводятся к нижнему регистру; короткие токены (<2) игнорируются.
- Индекс: реализован на собственной hash-таблице (SimpleHashMap) с цепочными списками.
- Булев поиск: операции AND/OR реализованы двумя указателями (двусвязное слияние отсортированных posting lists), NOT реализован как генерация complement списка по всем doc_id.
- Стемминг: простой эвристический стеммер для примера (не заменяет полноценные алгоритмы).

## Проверка корректности и верификация
- После токенизации проверьте верхнюю часть `results/frequencies.csv` — там должны быть самые частые термы.
- Постройте индекс и выполните несколько тестовых запросов в `boolean_search` (например, `term1 AND term2`, `NOT term`).
- Прогон `zipf_analyzer.py` даст численные параметры (a, C для Zipf; B для Mandelbrot), которые можно процитировать в отчёте.

## Рекомендации и дальнейшие улучшения
- Добавить фильтрацию HTML и нормализацию текста (удалять теги, ссылки, скрипты).
- Исключить стоп-слова и применять более качественный стеммер (Porter, Snowball).
- Для больших корпусов заменить простые хеш-таблицы на более эффективные структуры и сериализацию индекса (например, mmap или on-disk inverted index).

Если хотите — добавлю пошаговый пример с конкретным тестовым дампом и ожидаемыми результатами (top-10 частых слов и пример запроса).
